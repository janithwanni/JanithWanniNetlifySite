[
  {
    "path": "post/2021-02-08-stuck-with-plastic/",
    "title": "Stuck with plastic",
    "description": "Tidy Tuesday analysis of Plastic pollution",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2021-02-08",
    "categories": [],
    "contents": "\nThe background\nIntroduction\nTLDR: Sarah Sauve\ncollected the data for the city of St. John’s, to find out the companies\nand people that are contributing to the amount of plastic in the city.\nThe data was collected during October 2020.*\n\nData Dictionary\nThe plastic is categorized by recycling\ncodes.\nvariable\nclass\ndescription\ncountry\ncharacter\nCountry of cleanup\nyear\ndouble\nYear (2019 or 2020)\nparent_company\ncharacter\nSource of plastic\nempty\ndouble\nCategory left empty count\nhdpe\ndouble\nHigh density polyethylene count (Plastic\nmilk containers, plastic bags, bottle caps, trash cans, oil cans,\nplastic lumber, toolboxes, supplement containers)\nldpe\ndouble\nLow density polyethylene count (Plastic\nbags, Ziploc bags, buckets, squeeze bottles, plastic tubes, chopping\nboards)\no\ndouble\nCategory marked other count\npet\ndouble\nPolyester plastic count (Polyester fibers,\nsoft drink bottles, food containers (also see plastic bottles)\npp\ndouble\nPolypropylene count (Flower pots, bumpers,\ncar interior trim, industrial fibers, carry-out beverage cups,\nmicrowavable food containers, DVD keep cases)\nps\ndouble\nPolystyrene count (Toys, video cassettes,\nashtrays, trunks, beverage/food coolers, beer cups, wine and champagne\ncups, carry-out food containers, Styrofoam)\npvc\ndouble\nPVC plastic count (Window frames, bottles\nfor chemicals, flooring, plumbing pipes)\ngrand_total\ndouble\nGrand total count (all types of\nplastic)\nnum_events\ndouble\nNumber of counting events\nvolunteers\ndouble\nNumber of volunteers\nData preparation\n\n\nskimr::skim(tuesdata$plastic)\n\n\nTable 1: Data summary\nName\ntuesdata$plastic\nNumber of rows\n13380\nNumber of columns\n14\n_______________________\n\nColumn type frequency:\n\ncharacter\n2\nnumeric\n12\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncountry\n0\n1\n4\n50\n0\n69\n0\nparent_company\n0\n1\n1\n84\n0\n10823\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nyear\n0\n1.00\n2019.31\n0.46\n2019\n2019\n2019\n2020\n2020\n▇▁▁▁▃\nempty\n3243\n0.76\n0.41\n22.59\n0\n0\n0\n0\n2208\n▇▁▁▁▁\nhdpe\n1646\n0.88\n3.05\n66.12\n0\n0\n0\n0\n3728\n▇▁▁▁▁\nldpe\n2077\n0.84\n10.32\n194.64\n0\n0\n0\n0\n11700\n▇▁▁▁▁\no\n267\n0.98\n49.61\n1601.99\n0\n0\n0\n2\n120646\n▇▁▁▁▁\npet\n214\n0.98\n20.94\n428.16\n0\n0\n0\n0\n36226\n▇▁▁▁▁\npp\n1496\n0.89\n8.22\n141.81\n0\n0\n0\n0\n6046\n▇▁▁▁▁\nps\n1972\n0.85\n1.86\n39.74\n0\n0\n0\n0\n2101\n▇▁▁▁▁\npvc\n4328\n0.68\n0.35\n7.89\n0\n0\n0\n0\n622\n▇▁▁▁▁\ngrand_total\n14\n1.00\n90.15\n1873.68\n0\n1\n1\n6\n120646\n▇▁▁▁▁\nnum_events\n0\n1.00\n33.37\n44.71\n1\n4\n15\n42\n145\n▇▃▁▁▂\nvolunteers\n107\n0.99\n1117.65\n1812.40\n1\n114\n400\n1416\n31318\n▇▁▁▁▁\n\nSeveral plastic counts columns contain NAs. Is this due to a mistake\nin entering data or was there actually no plastics of that category to\nbe found? The most sound answer would be that there were no plastic\ncounts to be found which is quite unlikely. We can recalculate the grand\ntotal with these imputed 0s.\nAlso let’s focus on the year of 2019, since this study has finished\non the October 2020.\nInterestingly, there are parent companies called Grand Total and\nUnbranded, and a country called EMPTY as well.\n\n\nplastics <- tuesdata$plastics %>% \n  group_by(parent_company,country,year) %>% \n  mutate(across(empty:pvc,~replace_na(.x,0))) %>% \n  mutate(grand_total = sum(across(empty:pvc))) %>% \n  ungroup() %>% \n  filter(year == 2019) %>% \n  filter(country != \"EMPTY\" & \n           !(parent_company %in% c(\"Grand Total\",\"Unbranded\")) &\n           grand_total != 0) \n\n# %>% \n#   group_by(parent_company,year) %>% \n#   mutate(across(empty:pvc,mean)) %>% \n#   ungroup() %>% \n#   select(parent_company,hdpe:pvc) %>% \n#   distinct()\n\n\n\nWhat if instead of this approach we decided to model the plastic\ncounts using a Bayesian approach in order to impute the missing\nvalues?\nFor this article we will be using Stan as our Probabilistic\nProgramming Backend.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-07-30-communicating-results-from-an-analysis-a-short-thriller/",
    "title": "Communicating results from an analysis : A short thriller",
    "description": "One of my experiences in telling a story with data and the challenges associated with the journey",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-07-30",
    "categories": [],
    "contents": "\nIntroduction\nMost of us here have their own horror stories of statistical consulting written in different forms with different villains. This is one of my stories.\nThe names and places have been replaced with generic names that I came up to safeguard myself and everyone else who worked on this project with me.\nThe project\nI received the chance to work in this project through my girlfriend who was working in a production branch of a digital marketing agency (hereon referred to as EyeAngle). The project was simple. EyeAngle has been working with a client (hereon referred to as University of Liverpool) for a long time and the time had come for UoL to take in a new set of candidates into their environment. EyeAngle had worked on producing videos for social media content such as testimonials, teaser trailers and together with the parent company EyeAngle+ had run a campaign to recruit more candidates into UoL. Now the time has come to take the data that was collected in the process and analyse to present to the client of the success of the project. That was what I knew at first glance and I accepted the opportunity (this was in a period where everyone was quarantining in the middle of a pandemic, and I needed a project that was exciting as this to keep myself sane)\nNow before beginning on the task, I had to take a look at what sort of data was available for me. The supervisor of this project (hereon referred to as the Provost) informed me with the data that they have collected and what are the data that needs to be collected.\nThe data\nGoogly Data\nTo explain the sources of data, one must get to know the story behind how the data was collected. And here is where Google Analytics come into play.\nFrom the start of the campaign social media content has been posted on two platforms. Facebook and Instagram. Each social media content that was posted during the campaign had a link for potential candidates to sign up for the interview. The links had Google Analytics tracking attached with it (in the case of Instagram the links were all funnelled through the bio and in the case of Facebook the links with tracking codes were shortened and added as part of the post content). From that source a variety of data was collected. I didn’t receive access to the direct Google Analytics data because that would have been risky. Instead I received access to a Google Data Studio dashboard.\nNow from this dashboard I had to slowly collect data by setting the date and then downloading the CSV’s related to each of the three graphs that were present.\nAfter downloading a few files I realised that combining all of these into dataframe is going to be a massive collection of rbind codes. Therefore I googled a bit about how R gives out contents of a directory and lo and behold it was a list. quickly loads purrr library How convenient, I can simply put each of the three CSV’s that come under each date into a folder named with that date and then combine them all together by mapping through the list of files. The result was the following code.\n\n\napplication_conversion_rate <- map_dfr(dir_ls(\"raw_data/\"),\n                                       ~ read_csv(dir_info(.x)$path[1]) %>% \n                                         mutate(date = gsub(\"raw_data/\",\"\",.x)))\napplications <- map_dfr(dir_ls(\"raw_data/\"),\n                        ~ read_csv(dir_info(.x)$path[2]) %>% \n                          mutate(date = gsub(\"raw_data/\",\"\",.x)))\ncampaign_data <- map_dfr(dir_ls(\"raw_data/\"),\n                         ~ read_csv(dir_info(.x)$path[3]) %>% \n                           mutate(date = gsub(\"raw_data/\",\"\",.x)))\n\nwrite_csv(application_conversion_rate,\"combined_data/application_conversion_rate.csv\")\nwrite_csv(applications,\"combined_data/applications.csv\")\nwrite_csv(campaign_data,\"combined_data/campaign_data.csv\")\n\n\n\nAfter inspecting the final combined dataset I gained an understanding of what sort of data I have with me right now. In summary, I know in a particular date how many users clicked on the signup link on a SM post and then how many of them signed up using that link, in addition I also knew where these users came from and whether they came from an advertisement or simply from a SM post. So that was one dataset under my belt. I felt like a coding ninja using purrr, and as all good things go, my dopamine levels were about to make a massive hit after encountering the next source of data.\nSuper Data\nRemember how I mentioned paid advertisements? well there was the next catch. The Provost at the start of the project informed me that I will not be working alone in this project, and that there will be two people from EyeAngle working with me. Those two were simply my girlfriend and a really close friend of hers. Both of them were competent in R, Excel and were excellent content writers. But here I had the next problem that I never faced before.\nPitfall #1: Collaborative Google Slides with R\nAt the time being the only option was to use Rstudio cloud as an option for collaborative data analysis but the issue that came up next was that the final report needed to be made in Google Slides for several reasons such as convienience, collaborative real time editing and suggesting etc. This I knew would become a tedious two step process, as I would have to run the R script locally and then create the graph and ggsave it and then paste it on to the Google Slide and adjust it accordingly. There was no other option for me but to stick with this method.\nMoving forward from that the next task that the provost set forward for me was to collect data about the Facebook ad campaigns. There I learned about another tool on collecting data called Supermetrics.\nSupermetrics is free for use as a trial for 14 days and since I was supposed to submit this within less than 14 days I was able to use the free version as the provost told me. As per his instructions I set up a Google Sheet and then started by adding the Supermetrics addon to Google Sheets. It took me and the rest of the team two days to completely understand how the queries run from Supermetrics and what were the resulting data coming from. This was where knowing a bit about SQL helped to underestand what Supermetrics was trying to bring and from where. Finally after a few trial and error and a bundle of extra sheets in the Sheets workbook we managed to get the data into one place. ### Cleaning and wrangling\nAnd so began the journey into the muddy waters of data wrangling. It was tedious at first considering that we had to download Google sheets as csv files and then gather them all together into one place. Which brings us to the next pitfall that we faced in doing this project.\nPitfall #2: Lack of streamlined data pipelines\nWe didnt have a proper data pipeline to ensure that when we update the query in Supermetrics to refresh the data, the changes would be automatically reflected in our graphs. The intermediate step of having to download the data from Google Sheets locally as CSV files was the bottleneck of it all. It would have been much easier if we used the R package for handling Google Sheets and used it to auto update the graphs.\nPitfall #3: Data version control\nIn addition to these we also had the issue of data version control. Because what happened halfway through the entire scene was that we realized that there were missing pieces in the data because of a misconfiguration in the Supermetrics query. This would lead to the classic nightmare of having multiple datasets with the suffix if -1,-2,-new etc. This was a perfect recipe for disaster and the antidote would have been pretty obvious. Yes git would have been an option and that was where my stupidity came into place. I didn’t set up a local git repo to track the changes in the csv files so that we can revert back whenever we wanted. If we had switched to using Google Sheets package to get the synced data then version controlling would be given through the version history in Google Sheets. Regardless this was the biggest misconception I had back then. I would assume git repos need to be in github and local repos without a remote weren’t a thing. Butt after finishing the project only I realized how easy it would have been to manage the entire process if I had the common sense to just type git init and get on with the rest of the work.\nThe rest was it a blur of plots and late night calls to adjust different values according to the customer need and then the final presentation was done and everything was history.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-06-12-riddler-classic-how-long-will-the-bacterial-colony-last/",
    "title": "Riddler Classic : How long will the Bacterial Colony Last",
    "description": "Atttempt at solving Riddler problem with simulations",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-06-12",
    "categories": [],
    "contents": "\nRiddler Classic\nExcerpt from FiveThirtyEight\nFrom Adam Wagner comes a curious case of colonies:\nYou are studying a new strain of bacteria, Riddlerium classicum (or R. classicum, as the researchers call it). Each R. classicum bacterium will do one of two things: split into two copies of itself or die. There is an 80 percent chance of the former and a 20 percent chance of the latter.\nIf you start with a single R. classicum bacterium, what is the probability that it will lead to an everlasting colony (i.e., the colony will theoretically persist for an infinite amount of time)?\nExtra credit: Suppose that, instead of 80 percent, each bacterium divides with probability p. Now what’s the probability that a single bacterium will lead to an everlasting colony?\nQuestion 1\nDefining the problem\nLet’s take the case of one bacteria. Looking at the states that a bacteria can achieve it is clear that we can define a Bernoulli random variable as \\[Let\\ X\\ =\\ \\{The\\ event\\ that\\ a\\ bacteria\\ will\\ split\\ into\\ two\\} \\sim Bernoulli(0.8)\\] So first we sample from a Bernoulli variable. And if it comes true then we now have two bacteria.\nNow let’s consider the case of two bacteria where we have two trials. Guess a Bernoulli variable won’t cut it now (pun intended). Let’s define a new random variable as \\[Let\\ Y\\ =\\ \\{The\\ number\\ of\\ bacteria\\ that\\ will\\ split\\ into\\ two\\ from\\ n\\ bacteria\\}\\\\ Y\\ \\sim Binomial(n,0.8)\\\\ where\\ n\\ is\\ the\\ number\\ of\\ bacateria\\ at\\ a\\ point\\ in\\ time\\]\nDummy Simulation\n\n\n\nSo I thought of doing a simply simulation(Is this a Monte Carlo Simulation? I have no idea.) First I thought of simulating the bacteria growth for time_steps and then I repeat that again and again for evolutions times and from there I find the probability.\nTo find the probability I had to make an assumption, which was that the bacteria growths with upward trend will keep on going upward, which is satisfied as the probability of all bacteria dieing is quite low (i.e. for the case of let’s say 1000, then \\(X\\ \\sim Bin(1000,0.8)\\) and \\(Pr(X = 0) = ^{1000}C_{0}(0.8)^{0}\\times(0.2)^{1000} \\approx 0\\))\n\n\nevolutions <- 60\ntime_steps <- 60\nn_bactis <- 1\nfile_name_sim <- paste(here::here(),\"/content/post/\",\"riddler_2020_06_12_sim.csv\",sep=\"\")\nif(exists(file_name_sim)){\n  sim <- read_csv(file_name_sim)\n}else{\n  sim <- map_dfr(1:evolutions,function(x){\n  n_bactis <- 1\n    lapply(1:time_steps, function(y){\n      if(n_bactis >= (5*10^8)){\n        X <- n_bactis\n        n_bactis <- 1*10^9\n      }else{\n        X <- rbinom(1,n_bactis,0.8)\n        n_bactis <<- X * 2\n      }\n      list(evol = x,step = y,not_dead = X,tot = n_bactis)\n    }) \n  })\n  write_csv(sim,file_name_sim)\n}\n\ngood_evols <- sim %>% filter(step == time_steps & tot > 0) %>% nrow()\nprob <- good_evols / evolutions\n\nsim %>% \n  ggplot(aes(x=step,y=tot,group=evol))+\n  geom_point(size=0.2) + \n  geom_line(alpha=0.4) +\n  labs(x = \"Time Step\", y = \"Total Bacteria (capped)\",\n       title=\"Bacteria Simulation\",\n       subtitle = \"what is the probability that a single bacteria will lead to an everlasting colony?\",\n       caption = \"The total is capped at 10^9 to ensure that the simulation doesnt go beyond the integer limit\") +\n  annotate(\"label\",x=(time_steps/2),y=10^9,label = paste(\"Probability of spreading is =\",prob))\n\n\n\n\nThe answer to the question is that the probability is approximately 0.75 through simulations\nNow let’s try to mathematically derive it.\nQuestion 2\nSo the second question seems like its just another layer of the earlier simulation to make it adaptable for more probabilities. But in reality what they require is a mathematical proof of the answer.\nSecond Dummy Simulation\n\n\nevolutions <- 80\ntime_steps <- 80\nn_bactis <- 1\nprobs <- seq(0,1,by=0.1)\n\nfile_name_sim_2 <- paste(here::here(),\"/content/post/\",\"riddler_2020_06_12_sim_2.csv\",sep=\"\")\n\nif(exists(file_name_sim_2)){\n  sim_2 <- read_csv(file_name_sim_2)\n}else{\n  sim_2 <- map_dfr(probs,function(p){\n    map_dfr(1:evolutions,function(x){\n      n_bactis <- 1\n      lapply(1:time_steps, function(y){\n        if(n_bactis >= (5*10^8)){\n          X <- n_bactis\n          n_bactis <- 1*10^9\n        }else{\n          X <- rbinom(1,n_bactis,p)\n          n_bactis <<- X * 2\n        }\n        list(prob = p,evol = x,step = y,not_dead = X,tot = n_bactis)\n      }) \n    })\n  })\n  write_csv(sim_2,file_name_sim_2)\n}\nsim_2 <- sim_2 %>% mutate(prob = as.factor(prob),\n                      evol_prob = paste(evol,prob))\n\nsim_2 %>% \n  ggplot(aes(x=step,y=tot,group=evol_prob,color=prob))+\n  geom_point(size=0.2) + \n  geom_line(alpha=0.4) + \n  labs(x = \"Time Step\", y = \"Total Bacteria (capped)\", color = \"p\")\n\n\n\n\nThe relationship between \\(p\\) and the probability of a single bacteria creating a colony can be captured as follows.\n\n\ngood_evols <- sim_2 %>% filter(step == time_steps & tot > 0) %>% count(prob)\n# prob <- good_evols / evolutions\nprob_relation <- good_evols %>% mutate(perc = n / evolutions,\n                      prob = round(as.numeric(levels(good_evols$prob))[good_evols$prob],2))\nprob_relation %>% \n  ggplot(aes(x = prob,y = perc)) +\n  geom_point() +\n  stat_smooth(method=\"lm\") +\n  labs(title = \"Relationship between p and probability of a bacteria making a colony\",x = \"p\", y = \"Pr(bacteria making a colony)\")\n\n\n\n\n\n\nprob_lm <- lm(prob_relation,formula = perc ~ prob)\nsummary(prob_lm)\n\n\n\nCall:\nlm(formula = perc ~ prob, data = prob_relation)\n\nResiduals:\n        1         2         3         4         5         6 \n-0.094048  0.002738  0.112024  0.058810  0.005595 -0.085119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.8220     0.1648  -4.988 0.007556 ** \nprob          1.9071     0.2143   8.901 0.000881 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08963 on 4 degrees of freedom\nMultiple R-squared:  0.9519,    Adjusted R-squared:  0.9399 \nF-statistic: 79.22 on 1 and 4 DF,  p-value: 0.0008805\n\nSo the answer I got from this hack is\nProbability of one Bacteria becoming an entire colony =1.9071429*p-0.8220238\nThis is what I could mathematically derive as much as my tiny brain can allow,\nTo begin with when \\(n = 1\\) all bacteria must replicate,\nwhen \\(n = 2\\) still all bacteria must replicate, otherwise if only one replicates then we are still at 2 bacteria.\nwhen \\(n = 4\\), 3 or more should replicate,\nwhen \\(n = 6\\), 4 or more should replicate,\nwhen \\(n = 10\\), \\((\\frac{10}{2}) + 1\\) or more should replicate, when \\(n = N\\), \\((\\frac{N}{2}) + 1\\) or more should replicate\nTherefore the probability that with \\(p\\) probability of replicating, a single bacteria can make an entire colony is given by\n\\[ Pr(X_{1} = 1) + \\sum_{i=1}^N\\sum_{j=(\\frac{i}{2}) + 1}^i Pr(X_{i} = j) \\]\n\\[ Pr(X_{1} = 1) + \\sum_{i=1}^N\\sum_{j=(\\frac{i}{2}) + 1}^i {i \\choose j}\\times p^j \\times (1-p)^{i-j}\\]\nFrom here on I couldn’t find a simple expression for this so I let it be at this stage.\n\n\n\n",
    "preview": "post/2020-06-12-riddler-classic-how-long-will-the-bacterial-colony-last/2020-06-12-riddler-classic-how-long-will-the-bacterial-colony-last_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-06-08-tidytuesday-lessons-marble-racing/",
    "title": "TidyTuesday Lessons: Marble Racing",
    "description": "Failed attempts and learnt lessons from TidyTuesday on Marble Racing",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-06-02",
    "categories": [],
    "contents": "\n\n\n\nMoneyball in Marble Racing\nFor this weeks TidyTuesday dataset, the R4DS community has given a dataset on Marble Racing statistics. Specifically one season from Marbula One. Initially I had no idea on the context behind this, and therefore I began exploring a bit by watching the Youtube videos and reading up on the Wiki. My usual procedure for these datasets is to gain an understanding of the context by placing myself in the midst of the context and asking questions along the way.\n\n\n\nRemoving columns from the original dataset\n\n\nmarbles <- marbles %>% select(-notes,-host,-source)\n\n\n\nDo players play for more than one team?\n\n\n# Players that have played under different teams --------------------------\n\nmarbles %>% \n  group_by(marble_name) %>% \n  transmute(teams = n_distinct(team_name)) %>% \n  filter(teams > 1)\n\n\n# A tibble: 0 × 2\n# Groups:   marble_name [0]\n# … with 2 variables: marble_name <chr>, teams <int>\n\n# No marbles switched teams\n\n\n\nThis was fairly a stupid question that I asked myself before reading the Wiki properly. Regardless, since there was only season it’s highly unlikely that team players will be switching between teams.\nDoes track length affect the performance of a marble\n\n\n# Track Length affects player performance ---------------------------------\n# 32 players in total\n\nmarbles %>% \n  filter(str_detect(marbles$race,\"Q\",negate=TRUE)) %>% \n  mutate(track_length_m = factor(track_length_m)) %>% \n  group_by(track_length_m,marble_name) %>% \n  mutate(row_id = row_number()) %>% \n  ungroup()  %>% \n  ggplot(aes(x=track_length_m,y=time_s))+\n  geom_col(alpha=0.3) +\n  geom_point(size=0.4) + geom_line(aes(group=row_id),alpha=0.8) + \n  facet_wrap(~ marble_name,nrow=8) +\n  theme_bw()\n\n\n\n# track length doesnt affect performance\n\n\n\nSo from the above it was quite clear that the track length doesnt really affect the performance. This should have been obvious from the way each marble gets lifted back to her higher elevation, thereby restoring the gravitational energy within it.\nThe Moneyball Idea\nThe idea behind the moneyball theory was to use undervalued statistics to win the game. (That is an overly simplified statement. More info can be found here [http://thesportjournal.org/article/an-examination-of-the-moneyball-theory-a-baseball-statistical-analysis/])\nOne thing that baffled me with regards to the statistic that was used in moneyball was the way they decided on the statistic. I have seen several tutorials online that talked of using linear regression, but the actual thinking behind coming up with that statistic was still unclear to me.\nIn order to find a new statistic I decided to go further from the given data and collect some extra from the videos on the sites or tracks.\n\n\n# Adding aditional site statistics ----------------------------------------\n\nsite_data <- tibble(\n  site = unique(marbles$site),\n  turns = c(13,11,13,10,23,8,15,15),\n  splits = c(1,2,2,0,1,1,1,0),\n  site_avg_lap_time = c(33.58,36.55,26.94,31.29,41.12,24.11,33.95,30.31)\n)\n\nsite_data\n\n\n# A tibble: 8 × 4\n  site            turns splits site_avg_lap_time\n  <chr>           <dbl>  <dbl>             <dbl>\n1 Savage Speedway    13      1              33.6\n2 O'raceway          11      2              36.6\n3 Momotorway         13      2              26.9\n4 Hivedrive          10      0              31.3\n5 Greenstone         23      1              41.1\n6 Short Circuit       8      1              24.1\n7 Razzway            15      1              34.0\n8 Midnight Bay       15      0              30.3\n\nmarbles <- marbles %>% left_join(site_data)\nmarbles <- marbles %>% \n  mutate(time_per_turn = time_s / turns) %>% \n  mutate(diff_avg_site_lap_time =  site_avg_lap_time - avg_time_lap)\n\n\nglimpse(marbles)\n\n\nRows: 256\nColumns: 16\n$ date                   <chr> \"15-Feb-20\", \"15-Feb-20\", \"15-Feb-20\"…\n$ race                   <chr> \"S1Q1\", \"S1Q1\", \"S1Q1\", \"S1Q1\", \"S1Q1…\n$ site                   <chr> \"Savage Speedway\", \"Savage Speedway\",…\n$ marble_name            <chr> \"Clementin\", \"Starry\", \"Momo\", \"Yello…\n$ team_name              <chr> \"O'rangers\", \"Team Galactic\", \"Team M…\n$ time_s                 <dbl> 28.11, 28.37, 28.40, 28.70, 28.71, 28…\n$ pole                   <chr> \"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"…\n$ points                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ track_length_m         <dbl> 12.81, 12.81, 12.81, 12.81, 12.81, 12…\n$ number_laps            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ avg_time_lap           <dbl> 28.11, 28.37, 28.40, 28.70, 28.71, 28…\n$ turns                  <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 1…\n$ splits                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ site_avg_lap_time      <dbl> 33.58, 33.58, 33.58, 33.58, 33.58, 33…\n$ time_per_turn          <dbl> 2.162308, 2.182308, 2.184615, 2.20769…\n$ diff_avg_site_lap_time <dbl> 5.47, 5.21, 5.18, 4.88, 4.87, 4.86, 4…\n\n\n\nmarbles <- marbles %>% \n  mutate(race_type = str_match(marbles$race,\"S1([A-Z])\\\\d\")[,2],\n         race_number = as.numeric(str_match(marbles$race,\"S1[A-Z](\\\\d)\")[,2]))\n\ntotal_points <- marbles %>% \n  filter(race_type == \"R\") %>% \n  group_by(marble_name) %>% \n  summarize(total_points = sum(points,na.rm = T)) %>% \n  ungroup() %>% \n  arrange(desc(total_points)) %>% \n  mutate(marble_rank = row_number())\n\ngetPalette = colorRampPalette(brewer.pal(9, \"Set1\"))\n\nmarbles <- marbles %>% \n  left_join(total_points)\n  \ndata <- marbles %>% \n  filter(race_type == \"R\") %>% \n  group_by(race_number) %>% \n  arrange(time_s) %>% \n  mutate(rank = row_number()) %>% \n  ungroup() %>% \n  select(rank,race_number,team_name,marble_name,marble_rank)\n\nm_plot<- data %>% \n  ggplot(aes(x=race_number,y=rank,group=marble_name,color=marble_name)) +\n  geom_bump(size = 0.5, alpha = 0.3) +\n  geom_point(size=0.4) + \n  geom_bump(data=data %>% filter(marble_rank <= 5),alpha=1,size=1.5)+\n  geom_point(data=data %>% filter(marble_rank <= 5),size=1.9) + \n  geom_bump(data=data %>% filter(marble_rank <= 10 & marble_rank > 5),\n            alpha=0.6,size=1.0) + \n  geom_point(data=data %>% filter(marble_rank <= 5),size=1.4) +\n  scale_y_reverse() +\n  scale_color_manual(values = getPalette(length(unique(marbles$marble_name))))+\n  theme(legend.position=\"bottom\") +\n  guides(color=guide_legend(nrow=6)) + \n  labs(x = \"Race Number\", y= \"Rank in each race\",color=\"Team Name\",\n       title=\"Marbula One Player Performance\",\n       subtitle = \"Thicker lines mean higher final rank by points\")\n\nm_plot\n\n\n\n\nA simple bump plot to visualize the marble ranks with the new statistic\n\n\n# getPalette = colorRampPalette(brewer.pal(9, \"Blues\"))\nanot_data <- marbles %>% \n  filter(race_type == \"R\") %>% \n  filter(marble_rank <= 5 & race_number >= 7)\n\nm_plot_2 <- marbles %>% \n  filter(race_type == \"R\") %>% \n  ggplot(aes(x=race_number,y=time_per_turn,group=marble_name,color=marble_name)) +\n  #geom_point() + geom_line() \n  geom_bump(size=0.1,alpha = 0.2) + \n  geom_bump(data=marbles %>% \n              filter(race_type == \"R\") %>% \n              filter(marble_rank <= 5),alpha = 0.8,size=1) + \n  annotate(\"label\",\n           x=(anot_data$race_number),\n           y=(anot_data$time_per_turn+anot_data$marble_rank),\n           label=paste(anot_data$marble_rank,anot_data$marble_name))+\n  scale_y_reverse() + \n  theme(legend.position=\"bottom\") +\n  guides(color=guide_legend(nrow=4)) + \n  labs(x = \"Race Number\",y=\"Time taken per turn in track\",\n       title=\"Is time taken per turn in a track an indicator of being in the top five?\",subtitle=\"Distance between labels shows the final points gap\",\n       caption=\"The answer is no, taking more time does help in the end\") \nm_plot_2\n\n\n\n# ggsave(plot=m_plot_2,\"tidy_tuesday_2020_06_02_2.png\",width=297,height=210,units=\"mm\")\n\n\n\nA change of question\nNow suppose we feel like one of these statistics might be helpful to us. Now we need to formulate a way to decide whether a team selected by that statistic will win or not. For that I thought of modeling a simple bivariate distribution, preferrably that can model the probability that marble X and marble Y can win like this. $P_{X,Y}(X,Y) = {} $\nBut the problem now was how do I define winning? Would simply saying first place be enough? Then the probabilities will be tilted towards some marbles. Or would it be better to be broad and say winning can mean being in the top 3 positions. So now with that in mind, we have a discrete bivariate distribution that we want to find the exact or approximate probabilities of. Now the problem has become, how do we find the probabilities when we have really limited amount of data. One thought that came to my mind was to simulate a race and then estimate the probabilities of winning by Monte Carlo simulations. Simply put, we would similating many races with different team combinations and from them we try to calculate the relative probability that a team with X and Y has earned enough points to be in the top 3 positions.\nA Computational Block and ending notes\nHere is where i hit the biggest problem. Let me summarize\nThere are 32 marbles.\nA team can have 2 members.\nHow many team combinations are possible? \\(^{32}C_{2} = \\frac{32!}{2! * 30!} = 496\\)\nAnd then came another problem, my first idea was to find \\(^{496}C_{16}\\) combinations but there would be conflicts as team combinations may have the same player in different teams.\nFrom there on I thought of not going for the team combinations and instead thought of the player combinations in a race. Which ended up in an even worse dead end unfortunately as \\(^{32}C_{16} = 601080390\\) which meant that I’d have to simulate 601,080,390(six hundred one million eighty thousand three hundred ninety) races. (By simulating my intial thought was to regress the time that a player would take on a race and then sort by the time and award points accordingly. Here also I was lost as to whether the method would be feasible for such a large number of simulations.)\nWith that concludes my attempt at the tidy tuesday dataset, I learned a lot from it, but there were more questions unanswered than answered.\n\n\n\n",
    "preview": "post/2020-06-08-tidytuesday-lessons-marble-racing/2020-06-08-tidytuesday-lessons-marble-racing_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-06-08-generative-art-with-the-polar-coordinates/",
    "title": "Generative Art with the Polar Coordinates",
    "description": "\"Fun with Polar coordinates\"",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-05-29",
    "categories": [],
    "contents": "\nExplanation\nThis was a fun little experiment I wanted to do where I wanted to let a couple of children run around randomly in their lunch break maybe and plot their movements on a polar coordinates.\nSo the number of bois are set by n_bois and the number of seconds(or time points) they have their lunch break is set by recess_time. First I set the initial points of the bois as follows. These were simply set to be cartesian coordinates so that I can simply convert them to polar and see how it goes. \\[\nX_{i} = Unif(-100,100)\\\\  \nY_{i} = Unif(-100,100)\n\\] And then for each \\(t^{th}\\) time point I calculated the following values\n\\[\nangle_{t} = Unif(0,180)\\\\\nspeed_{t} = Unif(10,1000)\\\\\nt = 1,2,...,recess_time\\\\\nangles_{i} = Unif(-angle_{t},angle_{t})\\\\\nspeed_{i} = Unif(-speed_{t},speed_{t})\\\\\ni = 1,2,...,n\\_bois\n\\]\n\n\nlibrary(tidyverse)\nset.seed(42)\ntheme_set(theme_void())\n\nn_bois <- 100\na <- tibble(x=runif(n=n_bois,-100,100),\n            y=runif(n=n_bois,-100,100),\n            time=1,\n            tag=seq(1,n_bois))\n\nrecess_time <- 360\n\nradian <- function(degree){\n  (degree / 360) * 2 * pi  \n}\n\nfor(i in 1:recess_time){\n  angle_val <- runif(1,0,180)\n  speed_val <-runif(1,10,1000)\n  angles <- runif(n_bois,-angle_val,angle_val)\n  speed <- runif(n_bois,-speed_val,speed_val)\n  start_index <- length(a$x)-n_bois+1\n  end_index <- length(a$x)\n  b <- tibble(x = a$x[start_index:end_index] + speed * cos(radian(angles)),\n              y = a$y[start_index:end_index] + speed * sin(radian(angles)),\n              time = i,\n              tag=seq(1,n_bois))\n  a <- rbind(a,b)\n}\n\nggplot(a,aes(x,y))+\n  geom_point(aes(color=time,group=tag),\n                 size=0.5,alpha=0.8,show.legend = FALSE) +\n  geom_line(aes(color=time,group=tag),show.legend = FALSE) +\n  coord_polar() + \n  scale_color_gradient(low=\"#83a4d4\",high=\"#b6fbff\")\n\n\n\n\n\n\n\n",
    "preview": "post/2020-06-08-generative-art-with-the-polar-coordinates/2020-06-08-generative-art-with-the-polar-coordinates_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-06-08-first-attempt-at-generative-art/",
    "title": "First Attempt at Generative Art",
    "description": "\"Generative Art made from the video series: Functional Programming for \nartists by Danielle Navarro\"",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-05-27",
    "categories": [],
    "contents": "\nMy first Generative Art\nFrom the functional programming in R for artists (and scientists) video series by Danielle Navarro. More of her amazing work can be found at her website\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\nset.seed(42)\nn_scales <- 8\nall_scales <- runif(n_scales,0.95,1)\n# all_scales <- c(.8,.9,.95)\n\nn_angles <- 8\n# all_angles <- runif(min=-10,max=10,n=n_angles) \n# all_angles <- rbinom(n_angles,10,0.5)\nall_angles <- rnorm(n_angles,-10,sd=20)\n# all_angles <- c(-10,-5,0,5,10,15,20,25)\ncycles <- 10\nsplits <- 2\ncolor_increment <- 0.1\nlow_color <- \"#DAE2F8\"\nhigh_color <- \"#D6A4A4\"\nsize_decrement <- 0.1\n\nradian <- function(degree){\n  (degree / 360) * 2 * pi  \n}\n\nadjust_scale <- function(scale){\n  \n  new_scale <- scale * sample(all_scales,size=length(scale),replace=TRUE)\n  return(new_scale)\n}\nadjust_angle <- function(angle){\n  \n  new_angle <- angle + sample(all_angles,size=length(angle),replace=TRUE)\n  return(new_angle) \n}\n\nadjust_x <- function(old_x,scale,angle){\n  new_x <- old_x + scale * cos(radian(angle))\n  return(new_x)\n}\nadjust_y <- function(old_y,scale,angle){\n  new_y <- old_y + scale * sin(radian(angle))\n  return(new_y)\n}\nadjust_color <- function(color){\n  new_color <- color + color_increment\n  return(new_color)\n}\nadjust_size <- function(size){\n  new_size <- size - size_decrement\n  return(new_size)\n}\ngrow_from <- function(tips){\n  new_growth <- tips %>% \n    mutate(\n      old_x = new_x,\n      old_y = new_y,\n      scale = adjust_scale(scale),\n      angle = adjust_angle(angle),\n      new_x = adjust_x(old_x,scale,angle),\n      new_y = adjust_y(old_y,scale,angle),\n      color = adjust_color(color),\n      size = adjust_size(size)\n      )\n  return(new_growth)\n}\ngrow_sapling <- function(){\n  sapling <- tibble(\n    old_x = 0,old_y = 0,\n    new_x = 0,new_y = 1,\n    scale = 1,angle = 90,\n    color = 0,size = 1\n  )\n  return(sapling)\n}\n\ngrow_multi <- function(tips){\n  branches <- map_dfr(.x = 1:splits,\n                  .f= ~ grow_from(tips))\n  return(branches)\n}\ndraw_tree <- function(tree){\n  pic <- ggplot(tree,aes(x=old_x,y=old_y,xend=new_x,yend=new_y))+\n    geom_segment(aes(color=color,alpha=size),size=1,show.legend = FALSE)+\n    # coord_equal()+\n    theme_void()+\n    xlim(-10,10) + \n    ylim(0,10) + \n    # theme_black()+\n    scale_color_gradient(low=low_color,\n                         high=high_color) +\n    theme(plot.background = element_rect(fill = \"black\")) # +\n    # ggsave(paste(Sys.time(),\"ashtree.png\"))\n  return(pic)\n}\ngrow_tree <- function(){\n  tree <- map_dfr(accumulate(.x=1:cycles,\n                     .f= ~ grow_multi(.),\n                     .init = grow_sapling() ),\n                  ~.x)\n  return(tree)\n}\n\ndat <- grow_tree()\npic <- draw_tree(dat)\nplot(pic)\n\n\n\n\n\n\n\n",
    "preview": "post/2020-06-08-first-attempt-at-generative-art/2020-06-08-first-attempt-at-generative-art_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  },
  {
    "path": "post/2020-06-13-tidy-tueday-lessons-volleyball-matches/",
    "title": "TidyTueday lessons : Volleyball Matches",
    "description": "Learnt lessons from TidyTuesday on Volleyball matches",
    "author": [
      {
        "name": "Janith Wanniarachchi",
        "url": {}
      }
    ],
    "date": "2020-05-19",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(warning = FALSE)\n\n\n\nBeach Volleyball Pairs\nFor this weeks TidyTuesday dataset, the R4DS community has given a dataset on Volleyball Match statistics. This contained data in a wide format and the matches were mostly from the international FIVB tournaments but about 1/3 is from the US-centric AVP. I had only a vague idea about Beach Volleyball as I personally have played only regular volleyball(is there a term for that?) during PE period in school. So to get an idea I opened up youtube and searched beach volleyball and looked at the first match, which turned out to be a women’s beach volleyball match. And so I watched and I had a small question as to what pairs of players normally go well together as I heard the commentators explaining how these players played well with these partners and so on.\nLoad Data\n\n\n\nFiltering\nSince it was far less biased to look at the International Matches I had filtered it below.\n\n\n# select only international matches\nint_df <- vb_matches %>% \n  filter(circuit == \"FIVB\")\n\n\n\nHere I found a fun little use for functional programming as I wanted to get only the columns that had a complete rate of over 95%. I refrained from trying to handle the serves and attack columns due to this reason as their complete rate was quite low and to impute them would have caused even more of an hassle.\n\n\n#select only the columns that have a complete rate of over 95%\n\nsum_na <- compose(sum,is.na)\nis_not_na <- negate(is.na)\ncomplete_rate_mapper <- as_mapper(~ 100 - (sum_na(.x)/length(.x) * 100 ))\n\nint_df <- int_df[,names(keep(map(int_df,complete_rate_mapper),\n                             ~ .x > 95))]\n# skimr::skim(int_df)\n\n\n\nData Cleaning\nInitially I found having two countries of the players seperate odd and therefore thought of merging them only to realize that there were instances where the countries didn’t match. (which was quite odd to me). Regardless I decided to name those mixed and let it be.\n\n\n#merge player countries\nint_df <- int_df %>%\n  mutate(w_country = if_else(\n    w_p1_country == w_p2_country,w_p1_country,\"Mixed\"),\n    l_country = if_else(\n      l_p1_country == l_p2_country,l_p1_country,\"Mixed\")\n    ) %>%\n  select(-l_p1_country,-l_p2_country,-w_p1_country,-w_p2_country)\n\n\n\nThere were so many tournaments played in this dataset and to look at them all together made all the graphs really cluttered and therefore I thought to use only the top 10 tournaments with highest number of records.\n\n\n# recode gender variables\nint_df <- int_df %>% \n  mutate(gender = fct_recode(gender,\"Men\" = \"M\",\"Women\" = \"W\"))\n\n# limiting the problem to the top 10 tournaments\ntop_ten_tournaments <- int_df %>% select(tournament) %>% count(tournament) %>% arrange(desc(n)) %>% top_n(1) %>% .$tournament\nint_df <- int_df %>% filter(tournament %in% top_ten_tournaments)\n\n\n\nFinding the best player performances\nEven then there were too many, hence I decided to limit my scope to only the Mens team in USA. So to start finding the best player pair combinations the first thing I did was to count the Wins and Losses for the player combinations seperately. and then I combined them together and then got the percentages of wins and losses from total matches per player combination. And then I plotted a pair plot to see it.\n\n\n#for one country and one gender and for one tournament\nm_country <- \"United States\"\nm_gender <- \"Men\"\nm_w_df <- int_df %>% \n  filter(gender == m_gender & w_country == m_country) %>% \n  count(w_player1,w_player2) %>% \n  mutate(win_loss = \"Win\")\n\nm_l_df <- int_df %>% \n  filter(gender == m_gender & l_country == m_country ) %>% \n  count(l_player1,l_player2) %>% \n  mutate(win_loss = \"Loss\")\n\nm_df <- bind_rows(m_w_df %>% \n                    rename(player_1 = w_player1) %>%\n                    rename(player_2 = w_player2)\n                  ,m_l_df %>% \n                    rename(player_1 = l_player1) %>%\n                    rename(player_2 = l_player2))\nm_df <- m_df %>% \n  mutate(player_1 = gsub(\"([A-Z][a-z])*\\\\s([A-Z])[a-z]*\",\"\\\\1 \\\\2.\",m_df$player_1),\n         player_2 = gsub(\"([A-Z][a-z])*\\\\s([A-Z])[a-z]*\",\"\\\\1 \\\\2.\",m_df$player_2)) %>% \n  group_by(player_1,player_2) %>% \n  mutate(perc = n / sum(n)) %>% \n  ungroup()\n\nggplot(m_df %>% \n         mutate(pairs = row.names(m_df),\n                win_loss = factor(win_loss,levels=c(\"Win\",\"Loss\"))) %>% \n         gather(player,name,player_1,player_2),\n       aes(x=player,y=name,group=pairs)) +\n  geom_point(aes(color=win_loss,alpha=perc)) +\n  geom_line(aes(color=win_loss,alpha=perc)) + \n  labs(x = \"Player\",y=\"Name\",alpha=\"Ratio of Win/Loss\",color=\"Win/Loss\",title=\"Player Combinations in United States Men's Team in Gstaad Tournament for all years\") +\n  scale_color_brewer(palette=\"Set1\")\n\n\n\n\nAn approach with a different statistic\nNow there were several problems with the above statistics. One was that multiple colors on one another really didn’t convey which one was the best.\nInstead of using that confusing statistic I decided to use something different. I calculated the ratio of wins and losses as before and this time I calculated the difference between the ratios and then decided to categorize them by postive, negative or zero change.\nIn addition I also used some regular expressions to edit the names of the players so that I can put the names on the plot without cluttering the entire axis.\n\n\n# different approach\nm_w_df <- int_df %>% \n  filter(gender == m_gender & \n           w_country == m_country \n  ) %>% \n  count(w_player1,w_player2,name=\"n_wins\") %>% \n  rename(player_1 = w_player1) %>%\n  rename(player_2 = w_player2)\n\nm_l_df <- int_df %>% \n  filter(gender == m_gender & \n           l_country == m_country \n  ) %>% \n  count(l_player1,l_player2,name = \"n_loss\") %>% \n  rename(player_1 = l_player1) %>%\n  rename(player_2 = l_player2)\n\nm_df <- m_w_df %>% \n  full_join(m_l_df) \nm_df[is.na(m_df$n_wins),\"n_wins\"] <- 0\nm_df[is.na(m_df$n_loss),\"n_loss\"] <- 0\nm_df <- m_df %>% \n  mutate(total = n_wins + n_loss,\n         ratio_wins = n_wins / total,\n         ratio_loss = n_loss / total,\n         ratio_diff = ratio_wins - ratio_loss) %>% \n  mutate(good_bad = factor(case_when(\n    (ratio_diff > 0) ~ 0.8,\n    (ratio_diff < 0) ~ 0.2,\n    (ratio_diff == 0) ~ 0.5\n  ),levels=c(0.2,0.5,0.8),labels=c(\"Bad\",\"Neutral\",\"Good\"))) %>% \n  select(-ratio_wins,-ratio_loss,-total,-ratio_diff)\n  \nm_df <- m_df %>% \n  mutate(player_1 = gsub(\"([A-Z][a-z])*\\\\s([A-Z])[a-z]*\",\"\\\\1 \\\\2.\",m_df$player_1),\n         player_2 = gsub(\"([A-Z][a-z])*\\\\s([A-Z])[a-z]*\",\"\\\\1 \\\\2.\",m_df$player_2))\n\nm_df %>% \n  mutate(pairs = row.names(m_df)) %>% \n  gather(player,name,player_1,player_2) %>% \nggplot(aes(x=player,y=name,group=pairs)) +\n  geom_point(size=0.9) +\n  geom_line(aes(color=good_bad,alpha=as.numeric(good_bad)),size=1.0) + \n  labs(x = \"Player\",y=\"Name\",\n       color=\"Pair Performance\",\n       title=\"Player Combinations in United States Men's Team in Gstaad Tournament for all years\",\n       alpha = \"Pair Performance\") +\n  scale_color_brewer(palette=\"Set1\") +\n  scale_alpha_continuous(breaks=c(0.2,0.5,0.8))\n\n\n\n\nEnding notes\nAlthough the plot looked beautiful, it’s still not helpful at all. Maybe adding a plotly widget with tooltip text or annotating the best combinations might be helpful but I dont see any way to annotate without cluttering the plot.\n\n\n\n",
    "preview": "post/2020-06-13-tidy-tueday-lessons-volleyball-matches/2020-06-13-tidy-tueday-lessons-volleyball-matches_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-08-18T09:56:25+05:30",
    "input_file": {}
  }
]
